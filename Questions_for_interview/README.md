<h1 align="center">Questions for interview</h1>


## Description

Здесь будут вопросы, которые могу вам попасться на собесе.

 
# Questions

## Что такое Spark

BigData фреймворк с открытым исходным кодом, для распределенной пакетной и потоковой обработки неструктурированных и слабоструктурированных данных, входящий в экосистему Hadoop. 

В устройстве систем лежит 6 понятий: 

-   Cluster – все вычислительные средства (компьютеры или сервера), которые называются нодами. Каждая нода имеет свое железо и ОС. Нодами в кластере управляет cluster manager. 

-   Worker - Нода или сеть нод ресурсы которых используются для работы спарка.  

-   Executor - процесс ранящийся в JVM. На каждом воркере может быть несколько executors. Именно executor выполняет работу и процессы используя железо воркера. 

-   Master - машина откуда пишется код на выполнение. По сути там где вы пишете код 

-   Driver - Процесс который в зависимости от запуска может быть как на мастере так и на ноде. В драйвере есть метод main() который создает Spark-session что является логичесиким центром спарка и делает следующие вещи: 

    -   Нарезает код на job, stage, task 

    -   Создает Logical & Physical Plan 

    -   Координирует cluster manager для отправки задач на executors 

    -   Отслеживает процесс выполнения с его этапами 

-   Cluster Manager - отвечает за выделяемые ресурсы. Когда мы запускаем код то поднмается driver, который говорит сколько нужно executors с какими ресурсами для выполнения задачи и говорит об этом cluster manager. Есть несколько видов Cluster Manager: 

    -   Spark Standalone Cluster Manager. Самый обычный и поставляется с самим спарком. Не требует настройки)) 

    -Apache Mesos. Чуть сложнее и круче, но если предыдущий используется в тестовых целях, то этот я вообще не видел чтобы юзался. 

    -   Hadoop YARN. До прихода на рынок k8s был одним из лучших решений, однако из-за особенностей устройства сейчас юзается реже, например в облачных решениях(тот же AWS EMR). 

    -   k8s. Самый лучший и чаще всего будет он. Об устройстве k8s можно почитать в интернете, я лишь скажу что в рамках спарка один executor=один pod. Это добавляет дополнительную гибкость. 

 
## Что такое Map Reduce и в чём его суть

Технология для параллельной обработки данных в распределённых кластерах. Программы автоматически распараллеливаются и выполняются на разных нодах кластера, при этом исполнительная система сама заботится о деталях реализации. Может использоваться много для чего: индексация веб контента, кластеризация документов, машинное обучение итд. За основу взяты две процедуры map - применяет нужную функцию к каждому элементу списка и reduce - объединяет результаты работы map. 


3) Какие этапы есть в Map Reduce
4) Разница между Spark и Hadoop
5) RDD, DF, Dataset - разнциа между ними(в рамках PySpark только RDD и DF)
6) Что такое DAG и как это связано со спарком
7) Разница между действиями и трансформациями
8) Разница между узкими и широкими трансформациями
9) Что такое job, stage, task
10) Виды джойнов(не лефт, райт и иннер, а именно стратегий джойна в спарке)
11) Когда какой джойн юзать
11) Почему нельзя писать бездумно collect и прочие такие штуки
12) Разница между repartion и coalesce(просто ответа что coalesce ток уменьшает а repartition может и увеличивать мало, мало даже если человек скажет что repartition это 
shuffle всегда). Логику того как под капотом это работает. Плюсы и минусы
13) как работает partitionBy
14) PartitionBy в комбинации с repartition или coalesce
15) Как уменьшить количество shuffle
16) Оптимизации которые юзает каталист
17) Как решить проблему Data Skew и вообще что это такое
18) UDF и почему PySpark UDF это плохо. Чем заменить PySpark UDF
19) Delta Lake и почему он
20) Менеджмент памяти на уровне JVM+overhead
21) Менеджмент памяти на полном уровне(если это PySpark то там не ток JVM и overhead)
22) Parquet - что это(рассказать всё что знаешь)
23) Нюансы при чтении JSON
24) Нюансы при чтении CSV
25) Broadcast variables