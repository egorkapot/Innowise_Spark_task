<h1 align="center">Questions for interview</h1>


## Description

Здесь будут вопросы, которые могу вам попасться на собесе.

 
# Questions

## Что такое Spark

BigData фреймворк с открытым исходным кодом, для распределенной пакетной и потоковой обработки неструктурированных и слабоструктурированных данных, входящий в экосистему Hadoop. 

В устройстве систем лежит 6 понятий: 

-   Cluster – все вычислительные средства (компьютеры или сервера), которые называются **нодами**. Каждая нода имеет свое железо и ОС. Нодами в кластере управляет **cluster manager**. 

-   Worker - Нода или сеть нод ресурсы которых используются для работы спарка.  

-   Executor - процесс ранящийся в JVM. На каждом воркере может быть несколько **executors**. Именно **executor** выполняет работу и процессы используя железо воркера. 

-   Master - машина откуда пишется код на выполнение. По сути там где вы пишете код 

-   Driver - Процесс который в зависимости от запуска может быть как на мастере так и на ноде. В драйвере есть метод **main()** который создает **Spark-session** что является логическим центром спарка и делает следующие вещи: 

    -   Нарезает код на **job, stage, task**

    -   Создает **Logical & Physical Plan** 

    -   Координирует **cluster manager** для отправки задач на **executors** 

    -   Отслеживает процесс выполнения с его этапами 

-   Cluster Manager - отвечает за **выделяемые ресурсы**. Когда мы запускаем код то поднимается **driver**, который говорит сколько нужно executors с какими ресурсами для выполнения задачи и говорит об этом cluster manager. Есть несколько видов Cluster Manager: 

    -   Spark Standalone Cluster Manager. Самый обычный и поставляется с самим спарком. Не требует настройки)) 

    -   Apache Mesos. Чуть сложнее и круче, но если предыдущий используется в тестовых целях, то этот я вообще не видел чтобы юзался. 

    -   Hadoop YARN. До прихода на рынок k8s был одним из лучших решений, однако из-за особенностей устройства сейчас юзается реже, например в облачных решениях(тот же AWS EMR). 

    -   k8s. Самый лучший и чаще всего будет он. Об устройстве k8s можно почитать в интернете, я лишь скажу что в рамках спарка один executor=один pod. Это добавляет дополнительную гибкость. 

 
## Что такое Map Reduce и в чём его суть

Технология для параллельной обработки данных в распределённых кластерах. Программы автоматически распараллеливаются и выполняются на разных нодах кластера, при этом исполнительная система сама заботится о деталях реализации. Может использоваться много для чего: индексация веб контента, кластеризация документов, машинное обучение итд. За основу взяты две процедуры **map** - применяет нужную функцию к каждому элементу списка и **reduce** - объединяет результаты работы map. 


## Какие этапы есть в Map Reduce

-   Map – предварительная обработка входных данных в виде большого **списка** значений. При этом главный узел кластера (master node) получает этот список, **делит** его **на части** и передает рабочим узлам (worker node). Далее **каждый** рабочий **узел** применяет функцию **Map** к локальным данным и записывает результат в формате «**ключ-значение**» во **временное хранилище**. 

-   Combine - опциональный процесс, который ранится на каждой ноде **отдельно**. По сути это тот же редьюс до шафла который уменьшает количество данных которые будут закинуты в шафл 

-   Shuffle - когда рабочие узлы **перераспределяют** данные на основе **ключей**, ранее созданных функцией Map, таким образом, чтобы все **данные** одного **ключа** лежали на **одном** рабочем **узле**. 

-   Reduce – параллельная **обработка** каждым рабочим узлом каждой группы данных по порядку следования ключей и «**склейка**» результатов на **master node**. Главный узел получает промежуточные ответы от рабочих узлов и передаёт их на свободные узлы для выполнения следующего шага. Получившийся после прохождения всех необходимых шагов результат – это и есть решение исходной задачи. 


## Разница между Spark и Hadoop

Хадуповский MapReduce обрабатывает данные на базе **дискового хранилища**, в то время как спарк использует специальные примитивы для рекуррентной обработки в **оперативной памяти**. За счет этого вычислительные задачи реализуются на спарке значительно **быстрее**. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/hadoopvsspark.png" width="80%"></p>

## RDD, DF, Dataset - разнциа между ними(в рамках PySpark только RDD и DF)

На самом деле Dataframe и Dataset были построены на механизме RDD и получили новые функции например управление схемами = нет оптимизации для RDD + писать на RDD API сложнее. RDD используется только в одном случае: если файл txt или другой вообще не структурируемый. 

## Что такое DAG и как это связано со спарком

DAG (Направленный Ациклический Граф) в спарке - это штука которая показывает путь RDD от начала до конца и какие операции с ними происходят. 

## Разница между действиями и трансформациями

Трансформация всегда возвращает новый Dataset, DataFrame, RDD. Если это возвращает что то другое или вообще ничего - это действие 

Кроме того трансформация - это **ленивая** штука которая **не приводит к выполнению**. Спарк просто запомнит что нужно сделать, а **действие** как раз **запускает** трансформацию. 

## Разница между узкими и широкими трансформациями

### Узкие трансформации 

Узкие трансформации работают только в рамках каждого партишна не касаясь других. Количество входящих партишнов = количеству исходящих.  

-   Как правило такие трансформации быстрые. 

-   Не вызывают шафла = нет перехода по стейджам 

-   Map() & filter() - относятся к узким трансформациям 

<p align="center">
<img src="/../main/Questions_for_interview/Images/narrow-transformation.png" width="80%"></p>

### Широкие трансформации

Этот тип преобразования будет иметь входные партиции, которые могут относиться ко многим выходным партициям. Каждый партишн в родительском RDD может использоваться разными партишнами в дочернем RDD. Всегда вызывает **шафл данных**.

-   Медленные, скорость зависит от необходимых операций. Как было сказано выше - вызывает шафл данных. 

-   GroupByKey(), aggregateByKey(), aggregate(), join(), repartition() - примеры функций 

<p align="center">
<img src="/../main/Questions_for_interview/Images/wide-transformation.png" width="80%"></p>

## Что такое job, stage, task

-   Таск - выполняющаяся задача в спарке которая хранит в себе инструкции чтение данных, фильтрация итд). Нужно запомнить что **одна таска - одна партиция** 

-   Stage - каждая стейджа хранит в себе таски и каждая таска выполняет одинаковый сет инструкций. Прикол в том что **при шафле** условие перехода между стейджами - **обязательное**.  

-   Job - просто хранит в себе стейджи. Новая джоба создается когда запускается функция как **write()**. И джоба как раз и является действием, которое запускает трансформации = вариации всех стейджей и их преобразований. 

-   Application - хранит в себе джобы 


## Виды джойнов(не лефт, райт и иннер, а именно стратегий джойна в спарке)

В спарке есть 5 механизмов джойнов: 

-   Перемешанный хеш (Shuffle Hash Join) 

-   Широковещательный хеш (Broadcast Hash Join) 

-   Сортировка через слияние (Sort Merge Join) 

-   Декартов джойн (Cartesian Join) 

-   Широковещательный джойн вложенного цикла (Broadcast Nested Loop Join) 

### Broadcast Hash Join 

**Один** из двух входных наборов данных **транслируется всем исполнителям**. **Хеш-таблица** строится для **всех исполнителей из транслируемого набора данных**. Затем **каждый** партишн **не транслируемого набора** присоединяется независимо к другому набору данных. Он **не требует** этапа **перемешивания**. Единственное **требование** - это чтобы исполнители имели **достаточно памяти** для размещения транслируемого набора данных. Поэтому лучше избегать такого джойна если данные большие. 

### Shuffle Hash Join 

Предварительно **выравнивает** оба набора данных в соответствии со схемой разделения. Если один или оба набора **не соответствуют схеме** - используется **шафл** перед выполнением джойна. После обеспечения соответствия - используется хеш джойн из примера выше. **Требования к памяти** в этом подходе **меньше** чем в Broadcast Hash Join. Это связано с тем тчо тут хеш таблица строится на меньшем наборе входных данных. При этом эффективность будет ниже, если потребуется сделать шафл для одного или обоих входных наборов данных 

### Sort Merge Join 

Как и в шафле оба набора **выравниваются** в соответствии со схемой и в случае **несоответствия** - **шафлятся**. После обеспечения соответствия - используется стандартный подход Sort Merge Join. Он **менее эффективен чем хеширующие джойны**, но гораздо **ниже требования** по **памяти**. Как и в шафлящем - при операции перемешивания больше грузится на выполнение. 

### Cartesian Join 

Используется **исключительно для выполнения перекрестного джойна** (декартово произведение) который выводит все объединенные записи которые  возможны при объединении каждой записи из одного набора входных данных с каждой записью из другого набора входных данных. Сильно увеличивает количество выходных разделов.  

### Broadcast Nested Loop Join 

Один из наборов транслируется всем исполнителям. После этого каждый раздел не транслируемого набора джойнится к транслируемому с помощью стандартной процедуры. **Неэффективен** так как требует **вычислений циклом** + требует **много памяти** для трансляции. 

## Когда какой джойн юзать

Вообще есть такая штука как AQE которая сама решает когда какой джойн применить 

Spark выбирает конкретный механизм для выполнения операции Join, основываясь на следующих факторах: 

-   Параметры конфигурации 

-   Подсказки для Join (можно вручную ставить хинты)

-   Размер наборов входных данных 

-   Тип Join 

-   Эквивалентные или неэквивалентные джойны (Equi or Non-Equi Join) 

## Почему нельзя писать бездумно collect и прочие такие штуки

Коллект собирает **данные** со **всех RDD** со **всех нод** и возвращает на **драйвер**. Это норм применять его на маленьких датасетах, но если юзать его на больших датасетах то скорее всего получишь **OOM**. 

## Разница между repartion и coalesce(просто ответа что coalesce ток уменьшает а repartition может и увеличивать мало.Логику того как под капотом это работает. 

Сейчас будет затронута тема оптимизации и зачем вообще использовать **repartition**. Ее можно опустить и перейти сразу к [ответу на вопрос](https://github.com/egorkapot/Innowise_Spark_task/tree/main/Questions_for_interview#%D1%80%D0%B0%D0%B7%D0%BD%D0%B8%D1%86%D0%B0-%D0%BC%D0%B5%D0%B6%D0%B4%D1%83-repartition-%D0%B8-coalesce) в чем же разница.  

### Зачем использовать repartition 

Представьте что у вас есть файл в котором 150к строк. Весь файл помещается в **один блок данных** и это означает что работать с ним будет только **один воркер**. И если на входе может быть 2МБ то на выходе уже 1ГБ. И это все будет делать **один поток**.  

<p align="center">
<img src="/../main/Questions_for_interview/Images/repartition_vs_coalesce.png" width="80%"></p>

Мы знаем что количество блоков в **одном этапе** неизменно. В таком случае мы можем разорвать этап на два, добавив **repartition(N)**, который **делает шафл**, создавая на выходе **N блоков**, примерно равных по размеру. И так как он делает **шафл** - создается **новый этап**. 

<p align="center">
<img src="/../main/Questions_for_interview/Images/repartititon_new_stage.png" width="80%"></p>

Дальше нам нужно записать готовый результат в директорию, но понимаем что после перетасовки у нас 60 блоков и так как один блок выполняется одним потоком то у нас будет записано 60 файлов. В таком случае разработчики применяют метод **coalesce(N) который в отличие от repartition не приводит к перетасовке = не вызывает новый этап**, а просто складывает наши блоки в N количество блоков данных. Это приведет к тому что на этом этапе у нас будет **N воркеров**. И если мы решим записать все в **один файл**, то у нас будет всего **один воркер**, то ему будет **тяжело** все это **вычислять**. В таком случае перед сохранением с coalesce делают repartition, чтобы разбить последний этап на два: 

1.   Предпоследний который будет выполнять тяжелые вычисления 

2.   Последний который будет произведено сохранение нужных файлов 

#### Ложка дегтя и как фиксится 

Мы знаем что repartition распределяет данные **случайным образом**. И зная что паркет это колоночный формат для хранения файлов, в котором они еще и **сжимаются** - мы **ухудшаем сжимаемость данных**. Это **фиксится** добавлением **порядка** но внутри каждого блока данных 

<p align="center">
<img src="/../main/Questions_for_interview/Images/repartition_order.png" width="80%"></p>

Так как **сортировка** идет внутри **каждого блока**, то **перетасовки нет** и все работает в рамках одного исполнителя памяти. После такой сортировки **размер** исходного файла будет **меньше** так как мы улучшили сжимаемость 

### Разница между Repartition и Coalesce 

-   Repartition - это полная перетасовка = всегда вызывает шафл = новый стейдж 

-   Coalesce - складывание блоков и не вызывает новый шафл = не создает новый стейдж 



13) как работает partitionBy
14) PartitionBy в комбинации с repartition или coalesce
15) Как уменьшить количество shuffle
16) Оптимизации которые юзает каталист
17) Как решить проблему Data Skew и вообще что это такое
18) UDF и почему PySpark UDF это плохо. Чем заменить PySpark UDF
19) Delta Lake и почему он
20) Менеджмент памяти на уровне JVM+overhead
21) Менеджмент памяти на полном уровне(если это PySpark то там не ток JVM и overhead)
22) Parquet - что это(рассказать всё что знаешь)
23) Нюансы при чтении JSON
24) Нюансы при чтении CSV
25) Broadcast variables